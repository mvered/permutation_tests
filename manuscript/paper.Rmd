---
title: "Permutation Test Based U-Statistics: Simulations of Type I and Type II Error Rates"
author: "Michelle Vered"
affiliation: "Whiting School of Engineering, Johns Hopkins University"
contact: "Email: me@michellevered.com. Current affiliation Everybody Votes."
output: 
  bookdown::pdf_document2:
    template: article_template.tex
    keep_tex: true
    extra_dependencies: ["siunitx", "booktabs"]
    citation_package: natbib
bibliography: bibliography
biblio-style: bibstyle
abstract: "Permutation test based U-statistics can offer finite sample guarantees on Type I and Type II error rates and are minimax optimal in a variety of contexts. We focus on the application of permutation test based \\textit{U}-statistics for two-sample comparisons of medians as well as for independence testing in high dimensional multinomial problems. We attempt to validate the minimax framework for permutation test based \\textit{U}-statistics proposed by Kim, Balakrishnan, and Wasserman \\cite{kim2022}, by employing a series of simulations to investigate the performance of permutation test against other test statistics and compare Type I and Type II error rates under different sample sizes and different assumptions about the true underlying data distribution."
---

```{r setup, include=FALSE}

# packages
library(tidyverse)
library(quickcode)
library(kableExtra)
library(modelsummary)

# formatting settings
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
options(digits=1)
options(scipen=999)

knitr::opts_chunk$set(
  out.width = "0.9\\linewidth", 
  fig.align = "center",
  fig.pos = "H"  # Forces the figure to stay "Here"
)
```

# Introduction

This project explores the use of permutation tests in the context of two-sample and independence testing. As described in Kim, Balakrishnan, and Wasserman, the core paper which is the basis for this project, permutation tests can provide finite sample guarantees on the Type I error rate and can be minimax optimal in a variety of circumstances [@kim2022]. In particular, we focus on the use of permutation test based $U$-statistics as applied to a two-sample comparison of medians as well as in independence testing in high dimensional multinomial problems.

We utilize a data set on housing prices as the basis for various simulation studies to explore minimax optimality of permutation tests. We divide this data set into two groups and use these groups to represent the true population distributions of some random variable.

In one set of simulations, we sample from these two distinct populations. We then compare the frequency of Type II error rates (falsely failing to reject the null hypothesis) between a permutation test and a non-permutation test based approach. In a second set of simulations, we draw two samples both from the same underlying population group. We then compare the prevalence of Type I errors (falsely rejecting the null hypothesis) between permutation and non-permutation test based approaches.

# Methods

## Permutation Tests

In hypothesis testing, we calculate a test statistic based on the sample of data. Typically, we then assume that the under the null hypothesis, that test statistic should follow some specific distribution, such as a Student's $t$-distribution or a Chi-squared distribution. We choose a critical value as some point from that distribution, such as the value of the test statistic that corresponds to less than a 5% probability of observing a test statistic as extreme or more extreme than the critical value if the null hypothesis were true. We make a decision about whether to reject or fail to reject the null hypothesis by comparing our observed test statistic to that critical value.

However, the standard repertoire of hypothesis tests do not work in all situations. When dealing with small sample sizes, or when distributional assumptions required for parametric tests are not met (such as normality), one alternative we can turn to would be a permutation test. Permutation tests are a flexible alternative that allow us to create our own null distribution with which to compare our test statistic rather than assuming a specific distribution.

The basic idea of permutation testing is to randomly re-shuffle the labels of observations, then re-calculate the test statistic. This process is repeated many times to create a distribution for the test statistic under the null [@albert2015]. If we are comparing two samples, for example, if these two samples were drawn from the same underlying population distribution (i.e. the null was true), we would expect that the labels (sample 1 or sample 2) would not matter, so we want to calculate a permutation distribution for the test statistic under that assumption. We then select a critical value for our decision rule based on a quantile of the permutation distribution.

Formally, assume we have observations $X=X_1,...,X_n$ and $Y=Y_1,...,Y_m$ and define a test statistic $T(X,Y)$ which measures the difference between the two samples. For each permutation, select $n$ observations from the pooled data $X_1,...,X_n,Y_1,...,Y_m$ and label these $$X^*_{(1)}=X_{(1)_1}^*,...,X_{(1)_n}^*$$ and the remainder of observations as $$Y^*_{(1)}=Y_{(1)_1}^*,...,Y_{(1)_n}^*$$ then calculate a permuted test statistic $T^*_{(1)}(X^*,Y^*)$. Repeat this process $n$ times to get $$T^*_{(1)},...,T^*_{(n)}$$ (or repeat many times for a large sample where calculating every possible permutation of the data is computationally unfeasible). Choose a critical value $c^*$ as a quantile from the distribution of $T^*_{(1)},...,T^*_{(n)}$.

It is worth noting that bootstrapping is a related, but slightly different, re-sampling procedure for creating a distribution of the test statistic under the null. As compared with permutation testing, in a bootstrap approach, we re-sample from the original data, with replacement, to generate each new sample rather than permuting the labels of the original data [@janssen2003]. However, bootstrapping similarly creates a distribution for the test statistic for which we can find a particular quantile to serve as the critical value.

## Optimality

We explore several different approaches to hypothesis testing that allow us to illustrate the performance of the permutation-based approach to $U$-statistics. We focus on two applications in particular: testing whether two numeric samples come from the same underlying population with the same median (two-sample testing) and testing whether there is an association between two categorical variables (independence testing). In two sample testing, our null hypothesis is that the two samples come from the same underlying distribution with the same median. In independence testing, our null hypothesis is that there is no association between the categorical variables.

To assess the performance of different approaches to hypothesis testing, we are interested in measuring the Type I and Type II error rates produced by our estimators. Type I error rates occur when the null hypothesis is in fact true, but an estimator leads us to falsely reject the null. Type II error rates occur when the null hypothesis is in fact false, but an estimator fails to reject the null. The Type II error rates may occur when the test is underpowered and fails to detect an effect.

In practice, understanding the power of statistical test can be very important, as statisticians are often called upon to help design studies and determine optimal sample sizes for experiments or models to reach sufficient statistical power. Overestimating the sample size needed can lead to unneeded costs in the study implementation/data collection phase, but underestimate the required sample size can result in wasted effort running a study that produces only inconclusive results.

Previous work has explored the minimax optimality of permutation tests in large samples [@robinson1952] and for simple test statistics such as the $t$-statistic and $F$-statistic [@janssen2005]. A primary contribution of Kim paperis to provide a non-asymptotic framework to analyze the minimax Type II error of the permutation test  [@kim2022]. Their framework is particularly useful for small sample sizes. In one set of Monte Carlo simulations, they focus on simulations with $n_1=n_2=100$.

We can define a minimax optimal estimator as one the minimizes the maximum risk. Formally, the maximum risk of an estimator $\widehat{\theta}$ can be expressed as $$\bar{R}(\widehat{\theta})=\sup_{\theta} R(\theta, \widehat{\theta})$$ and a minimax optimal estimator will be one such that $$\sup_{\theta}R(\theta,\widehat{\theta})=\inf_{\tilde{\theta}} \sup_\theta R(\theta, \tilde{\theta})$$ where the infimum is over all estimators $\tilde{\theta}$ [@wasserman2010].

In practice, a hypothesis testing approach would be minimax optimal in terms of the Type II error rate when the decision rule it produces minimizes the likelihood of failing to reject the null hypothesis when the null is in fact false, as compared to all other decision rules. This "worst case" situation would be most likely to occur when there is a very small difference between the expected distributions under the null and alternate hypotheses.

As an added step in this project, we also run simulations to measure the Type I error rate. Finite sample guarantees on the Type I error rate are much more well-established, as long as the exchangeability assumption is met. These results are covered in older elements of the literature on permutation tests [@hoeffding1952].

In our simulations, we will use the King County, Washington housing price data as if it were the underlying true population distribution, so simulations will not necessarily be measuring the Type I and Type II error rates under the worst case, closest possible degree of similarity between the null and alternate hypotheses. We also won't be comparing permutation test based approaches with every other possible hypothesis test. But we will be able to measure the Type II minimax optimality of permutation tests in the sense of looking across many different samples from the population and looking at the frequency of Type II errors across the "unluckiest" samples with the worst possible configuration of observations selected to be in the sample.

## U-Statistics

Here we specifically focus on the use of permutation-based approaches with a class of statistics known as $U$-statistics. We can define $U_n$ as a $U$-statistic with kernel $h$ of degree $m$ as $$U_n = \binom{n}{m}^{-1} \sum_{(i_i,...,i_m)\in I_{n,m}} h(X_{i_1},...,X_{i_m})$$ where $h$ is a symmetric function over $m$ arguments and we are averaging the outcomes $h(X_{i_1},...,X_{i_m})$ across all possible ordered tuples $I_{n,m}=\{(i_1,...,i_m): 1\leq i_1 <...<i_m \leq n \}$ [@chen2005], [@wasserman2010]. Many estimators can be written to fit the format of this class of statistics, such as the sample variance.

## Two Sample Testing

In two-sample testing, we consider two samples of numeric data. Under the null hypothesis, we assume both samples come from the same underlying population distribution.

There are many options for performing this type of test. $t$-tests are a traditional, parametric approach but assume the underlying data is normally distributed. The Mann-Whitney $U$-test, also called the Wilcoxon rank-sum test, is a non-parametric alternative. The Mann-Whitney $U$-test is a test of both distribution shape and location (the central tendency/median). If we are able to assume that one sample is a shifted version of the distribution of the other sample, then the Mann-Whitney $U$-test is a test that directly allows us to test for a difference in medians [@hart2001]. The Mann-Whitney $U$-statistic is calculated by first defining a rank function that orders the data in both samples $$r(x_i)=\text{rank of }x_i \text{ in }x_1,x_2,...,x_{n_X},y_1,y_2,...,y_{n_Y}$$ Then the test statistic is the smaller of $$U_1=n_Xn_Y + \frac{n_X(n_X+1)}{2}-\sum_{i=1}^{n_X} r(x_i)$$ and $$U_2 = n_X n_Y + \frac{n_Y(n_Y+1)}{2}-\sum_{j=1}^{n_Y} r(y_j)$$

```{r mannWhitneyStat}

# calculate mann-whitney u statistic
mann_whitney_u <- function(x, y){
  n_x <- length(x)
  n_y <- length(y)
  pooled <- c(x, y)
  
  ranks <- rank(pooled, ties.method = "average")
  #u_x <- sum(ranks[1:n_x]) -  (n_x*(n_x+1))/2
  #u_y <- sum(ranks[(n_x+1):(n_x+n_y)]) - (n_y*(n_y+1))/2
  u_x <- n_x*n_y + (n_x*(n_x+1))/2 - sum(ranks[1:n_x])
  #u_y <- n_x*n_y + (n_y*(n_y+1))/2 - sum(ranks[(n_x+1):(n_x+n_y)])
  return(u_x)
}
```

While the Mann-Whitney $U$-test is non-parametric in that it does not assume a normal distribution or any other specific distribution for the underlying data, in the standard test we do assume that the distribution of the $U$-statistic under the null can be approximated by the normal distribution. Critical values are then taken that correspond to probabilities of the normal distribution. This approximation tends to work with sufficiently large sample sizes.

For a permutation test based Mann-Whitney $U$-statistic, the $U$-statistic itself would be calculated in exactly the same way, but the reference distribution for finding the critical value would be created through permutation.

## Independence Testing

In independence testing, we consider two categorical variables. Under the null hypothesis, we assume there is no association between these variables. Here we specifically look to a binary categorical variable and a second categorical variable that has a large number of categories.

A standard approach to multinomial independence testing is to use the Chi-squared test. However, this approach has a number of drawbacks. Kim et. al. highlight the particular utility of permutation testing in high-dimensional multinomial settings, when the the number of categories is large and may even be larger than the number of observations in the sample [@kim2022]. The drawbacks of traditional tests for these high dimensional multinomial settings was investigated in Balakrishnan, who suggest permutation tests as a potential solution to explore [@balakrishnan2018].

An alternate approach Berrett separately considers the optimality of $U$-statistic based permutation tests for independence testing and arrived at similar results for minimax optimality, but by taking a different mathematical approach using different assumptions on smoothness of permutation distributions [@berrett2020].

Kim et. al. propose a new $U$-statistic for independence testing in high dimensional multinomial settings [@kim2022]. Let $p_y$ and $p_z$ be multinomial distributions on a discrete domain $\mathbb{S}_d$ with kernel $$h_{ts}(y_1,y_2;z_1,z_2)=g(y_1,y_2)+g(z_1,z_2)-g(y_1,z_2)-g(y_2,z_1)$$ defined by the following bivariate function $$g_\text{Multi}(x,y)=\sum_{k=1}^d \mathbb{I}(x=k)\mathbb{I}(y=k)$$ and the corresponding $U$-statistic $$U_{n_1,n_2}=\frac{1}{(n_1)_{(2)}(n_2)_{(2)}} \sum_{(i_1,i_2)\in i^{n_1}_2} \sum_{(j_1,j_2)\in i^{n_2}_2} h_{ts}(Y_{i_1},Y_{i_2};Z_{j_1},Z_{j_2})$$

A unique contribution of this project is implementing calculation of this test statistic into C++/Rcpp.

# Data Exploration

## Overview

We now turn to an exploration of the data set used for running the simulations in this project.

```{r readInData}

# read in data
data <- read.csv("../data/home_data.csv") |>
  
  # fix date column formatting
  mutate(date = paste0(substr(date,1,4),"-",
                       substr(date,5,6),"-",
                       substr(date,7,8)),
         date = as.Date(date),
         
  # zip is categorical
  zipcode = factor(zipcode),
  
  # waterfront is binary
  waterfront = case_when(
    waterfront == 0 ~ FALSE, 
    waterfront == 1 ~ TRUE),
  
  # these are ordinal data
  view = factor(view, levels = seq(0,4,by=1), ordered = TRUE),
  condition = factor(condition, levels = seq(1,5,by=1), ordered=TRUE),
  grade = factor(grade, levels=seq(1,13,by=1), ordered=TRUE),
  
  # id is a label
  id = as.character(id),
  
  # convert price to be in thousands of dollars
  price = price/1000,
  
  # create a couple additional binary variables
  has_basement = case_when(
    sqft_basement > 0 ~ TRUE, 
    sqft_basement == 0 ~ FALSE),
  was_renovated = case_when(
    yr_renovated == 0 ~ FALSE,
    yr_renovated > 0 ~ TRUE),
  )
```

The data set we have selected for this project provides information on the sale price and other characteristics of houses which were sold in King County, Washington (the Seattle area). The data set includes `r nrow(data)` observations on homes sold between 2014 - 2015. Each observation in the data set represents one sale of a property.

After importing, we applied some basic data cleaning by re-classifying the data types for each of the columns when necessary and converted prices to be in thousands of dollars. We also engineered two Boolean variables to convert column that records the square footage of a basement into a binary classification, and did the same for a column indicating the year a house was renovated. Overall summary statistics for this data set are presented below:

```{r initialExploration, tab.cap="Housing data set summary statistics"}

# summary statistics for data set overall
datasummary_skim(data, output = "kableExtra", escape = FALSE,
                 title = "Summary statistics for housing data set") |>
  kable_styling(latex_options = c("striped", "hold_position")) 
```

These summary statistics show a relatively clean data set already, with no missing values in any of the columns.

We will focus on only a subset of the columns available in this data set. Of particular interest are the columns:

-   *has_basement*: Boolean variable indicating if the house has a basement or not

-   *price*: numeric variable indicating the sale price of the home, in thousands of dollars

-   *zipcode*: categorical variable

Since this paper focuses on the application of permutation-test based $U$-statistics in the context of two sample testing and independence testing, we are interested in dividing the data set into two samples. We can do that using the *has_basement* column. While *waterfront* is also a Boolean variable available in the data set, the very small overall number of waterfront properties makes this column less suitable for comparison of hypothesis testing approaches. We also considered splitting the data based on the was *was_renovated* column, but the *has_basement* column seemed to better fit the assumption of a shifted distribution required for the Mann-Whitney $U$-test, so we decided to focus there for the purpose of comparing approaches to two-sample and independence testing.

Diving the data set based on homes with and without a basement, we have:

```{r basementSummaryStats}

# total counts on properties with/without basement
n_has_basement <- data |> filter(has_basement == TRUE) |> nrow()
n_no_basement <- data |> filter(has_basement == FALSE) |> nrow()
```

-   **Population 1:** `r prettyNum(n_has_basement, big.mark=",")` properties with basements (`r round(n_has_basement/nrow(data)*100,1)`% of the data set)

-   **Population 2:** `r prettyNum(n_no_basement, big.mark=",")` properties with basements (`r round(n_no_basement/nrow(data)*100,1)`% of the data set)

## Price

We can compare quantiles for the distribution of prices between these two types of properties.

```{r basementTable}

# create table with quantiles for properties with/without basement
data |> 
  group_by(has_basement) |> 
  summarize(`Price Q25` = quantile(price, 0.25),
            `Median Price` = quantile(price, 0.5),
            `Price Q75` = quantile(price, 0.75)) |>
  kable(caption = "Price for houses with and without a basement", 
        format = "latex",
        booktabs = TRUE, 
        escape = TRUE)
```

It seems clear from examining the quantiles of each population's distribution that homes with and without basements represent two distinct populations with different distributions of the price variable.

We can also visualize the whole distribution of prices for these two groups of houses.

```{r histogramBasementPrice, fig.width=9}

# histogram showing distribution of price for basement/no basement houses
data |>
  ggplot(aes(x = price, fill=has_basement)) +
  geom_histogram(bins = 40, color="white") +
  facet_wrap(~has_basement, 
             labeller = labeller(has_basement = c(`FALSE` = "No Basement", 
                                                  `TRUE` = "Has Basement"))) +
  scale_x_continuous(labels = scales::dollar_format(scale = 1e-3, suffix = "M")) +
  labs(x = "Price", y = "Count of Observations", 
       fill = "Has Basement") +
  theme(panel.spacing = unit(2, "lines"))
```

There a few outliers - extremely expensive homes - which shift the histograms and make it difficult to see prices clearly for houses under \$2 million. It is also harder to directly compare the two distributions since there are differing numbers of observations in the two groups.

To hone in on a comparison of price distributions, we can generate a new overlapping density plot and exclude outliers (the largest 1% of home prices).

```{r densityPlotBasementPrice}

# threshold for outliers to hide on plot so we see bulk of points more clearly
outlier_threshold <- quantile(data$price, .99)

# zoomed in histogram showing distribution of price for basement/no basement houses
data |>
  filter(price < outlier_threshold) |>
  ggplot(aes(x = price, fill=has_basement, color=has_basement)) +
  geom_density(alpha=0.4) +
  scale_x_continuous(labels = scales::dollar_format(scale = 1e-3, suffix = "M")) +
  labs(x = "Price", y = "Density",fill="Has Basement",color="Has Basement") +
  theme(panel.spacing = unit(2, "lines"))
```

From this density plot, and the quantiles for each distribution provided above, it appears as though the distribution of prices for homes with a basement may be a shifted version of the distribution of prices for homes without a basement. This is an ideal set-up for the use of a Mann-Whitney $U$-test to compare medians, as discussed in more detail in the methods section.

## Zip Codes

We can also compare the distribution of zip codes between the two populations. Counts of homes per zip code can be thought of as draws from a multinomial distribution.

To better understand how the distributions of zip codes might compare in the two populations, homes with and without basements, we can examine a bar chart. The bar chart below visualizes the percentage of homes in each population that are located in a particular zip code.

```{r zipByBasement, fig.height = 20, fig.width=10}

# get percentage of each group per zip code
zip_by_basement <- data |>
  group_by(has_basement, zipcode) |>
  summarize(count = n()) |>
  ungroup() |>
  group_by(has_basement) |>
  mutate(percentage = (count/sum(count)))

# coordinate flipped column chart of zip code distributions
zip_by_basement |>
  ggplot(aes(x = zipcode, y=percentage, fill=has_basement)) +
  geom_col(width = 0.7, position = position_dodge(width = 0.8)) +
  scale_y_continuous(labels = scales::percent) +
  coord_flip() +
  labs(x = "Zip Code", y = "% of Observations in Population Group", fill = "Has Basement") +
  theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 26),
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 15)
  )
```

It seems quite clear from this bar chart that the two populations follow different underlying distributions. Since we are treating the data set as our underlying populations, clear differences on the graph should be enough to tell us about how zip code distributions compare between two populations.

This data represents an ideal scenario to examine independence testing for high dimensional multinomial distributions since there are `r length(unique(data[,"zipcode"]))` distinct zip codes included in the King County, Washington data set.

# Results

We now walk through the results of various simulation studies to assess the performance of the permutation test approach.

```{r defineData}

# set up for sampling procedure
with_basement <- data |>
  filter(has_basement == TRUE)
no_basement <- data |>
  filter(has_basement == FALSE)

```

## Two-Sample Testing

We first perform two-sample testing utilizing a Mann-Whitney $U$-statistic to compare the median and shape of two distributions for the price variable. 500 simulations and 500 permutations per simulation were used to create the permutation distributions.


### Type II Error

We start with data We'll start with a scenario where the null hypothesis is actually false because we are drawing the two samples from separate populations - homes with and without basements. For each simulation, we record whether our test correctly rejects the null hypothesis or fails to do so at a significance level of $\alpha = 0.05$, checking both a standard Mann-Whitney $U$-test and one utilizing a permutation distribution.

`
```{r twoSampleType2Table}
two_sample_sims.type_2 <- readRDS("../results/two_sample_sims_type_2.rds")

two_sample_sims.type_2 |>
  kable(caption = "Type II Error Rate of Two-Sample Testing Using a Mann-Whitney U-Statistic", 
        format = "latex",
        booktabs = TRUE, 
        escape = TRUE)
```

From these results, we see that the permutation based approach performs similarly to the standard Mann-Whitney U test in terms of minimizing the Type II error rate across all sample sizes.

### Type I Error

Now we run simulations where the null hypothesis should in fact be true. We are drawing both samples from the same underlying population, where houses do not have a basement.

```{r twoSampleTypeITable}

# two sample type I error table
two_sample_sims.type_1 <- readRDS("../results/two_sample_sims_type_1.rds")

two_sample_sims.type_1 |>
  kable(caption = "Type I Error Rate of Two-Sample Testing Using a Mann-Whitney U-Statistic", 
        format = "latex",
        booktabs = TRUE, 
        escape = TRUE)
```

From these simulations, it appears the permutation test performs better than the standard Mann-Whitney U test that uses a normal approximation for the test statistic distribution terms of the Type I error rate across all sample sizes.

The permutation based approach, at least for the relatively small sample sizes considered, appears to match the performance of the standard Mann-Whitney $U$-test as far as Type II error rates are concerned while reducing Type I error rates. This highlights the value of the permutation-based approach as a tool for two-sample hypothesis testing in small sample sizes with non-normally distributed data.

## Independence Testing

Next we turn to independence testing. We are testing the null hypothesis that there is no relationship between whether or not a house has a basement and which zip code it is located in. This represents a high-dimensional setting, given the high number of zip codes and anticipated sample sizes we will study ($n\leq 100$).

```{r zipsBackToStrings}

# conversion for ease of testing
no_basement <- no_basement |>
  mutate(zipcode = as.character(zipcode))
with_basement <- with_basement |>
  mutate(zipcode = as.character(zipcode))

```

The Chi-squared test can be used for this type of testing procedure. However, p-values for the Chi-squared distribution assume certain minimum expected counts per cell. In a larger Chi-squared table, the test assumes that all expected counts must be at least 1 and no more than 20% of cells can have expected frequencies less than 5, or else the p-value may not be valid. We anticipate this to be a major issue in this circumstance and will likely lead to the presence of additional Type I and Type II errors when the Chi-squared test is applied. We perform the Chi-squared test anyway in order to highlight the utility of the alternative permutation-based approach using a $U$-statistic proposed by Kim et. al. [@kim2022].

For the sake of comparison, we also report out Type I and II error rates for a Chi-squared test performed with simulated p-values (generated through Monte Carlo simulation using a bootstrapping procedure). Bootstrapping is similar to the permutation-based approach for calculating p-values. However, the Chi-squared test with simulated p-values still differs significantly from the Kim approach [@kim2022] in that it is not a $U$-statistic and the test statistic follows a different design/formula from the one specifically proposed by Kim for high-dimensional multinomial testing. We include it to provide another reference point for comparison of the new approach, since p-values calculated through reference to the standard $\chi^2$ distribution are expected to be relatively inaccurate for this use case.

Due to computational limitations, the number of simulations, the number of permutations, and the sample sizes tested are smaller than what we ideally would like to test. 100 simulations were implemented with 100 permutations per simulation used to create the permutation. Some parallelization has been implemented, but further simulations would benefit from another look at parallelizing and optimizing the code as well as cloud computing resources with a greater number of CPUs.




### Type II Errors

We now perform independence testing across a number of simulations, comparing the Type II error rates generated through use of a $\chi^2$ test with standard p-values (calculated through reference to a $\chi^2$ distribution), a $\chi^2$ test with simulated (Monte Carlo) p-values, through the multinomial $U$-statistic permutation test proposed Kim et. al. [@kim2022].

```{r tableIndepedenceType2Error }

# independence testing type 2 error table
independence_sims.type_2 <- readRDS("../results/independence_sims_type_2.rds")

independence_sims.type_2 |>
  kable(caption = "Type II Error Rate of Multinomial Independence Testing", 
        format = "latex",
        booktabs = TRUE, 
        escape = TRUE)
```

The error rate for all methods is high, which is predictable considering this is a high-dimensional multinomial setting and the sample sizes are quite small. No matter the method, it is difficult to correctly detect from just 10 or 20 observations that distribution of zip codes is related to having a basement, since there are `r length(unique(data[,"zipcode"]))` distinct zip codes included in the King County, Washington data set. We would expect the prevalence of Type II errors to decline across all methods as the sample size increases. 

However, from these limited simulations, it appears the permutation test performs better than the Chi-squared test (both versions) in terms of the Type II error rate.

### Type I Errors

```{r tableIndepedenceType1Error }

# independence testing type 2 error table
independence_sims.type_1 <- readRDS("../results/independence_sims_type_1.rds")

independence_sims.type_1 |>
  kable(caption = "Type I Error Rate of Multinomial Independence Testing", 
        format = "latex",
        booktabs = TRUE, 
        escape = TRUE)
```

From these simulations, it appears the permutation test performs similarly (or perhaps slightly worse) than the Chi-squared tests. More precision in these estimates might be possible with a larger number of simulations. 

# Conclusions

Our simulations show that the permutation-test based approach is a useful tool when it comes to hypothesis testing for small sample sizes. The theoretical guarantees on error rates given by Kim et al. are evident in the results of the simulations.

We first looked at two-sample testing, comparing the medians of two numeric samples of data. At least for the small sample sizes considered, the Type II error rate for p-values obtained through permutation versus through approximation to the normal distribution are roughly similar. However, the permutation approach appeared to reduce Type I error rates. This highlights the value of the permutation-based approach as a tool for two-sample hypothesis testing in small sample sizes with non-normally distributed data.

We then looked at independence testing for two categorical variables. We try to infer a general trend from the few sample sizes examined within the scope of this paper and identify the multinomial $U$-statistic as a potentially viable option for independence testing with categorical variables.

However, we do note that permutation testing has some drawbacks, namely the computational complexity. Permutation testing is not a very practical approach for larger data sets due to the time required to run. Even here with very small sample sizes, the simulations took a long time to run, particularly when calculating the multinomial $U$-statistic proposed by Kim et al. This $U$-statistic was very slow to compute, a permutation distribution of this statistic even more so. Future implementations of that statistic might wish to consider optimizations to code to better take advantage of parallelization and also to use cloud computing tools with a larger number of CPUs, since permuted test statistics can be calculated simultaneously rather than sequentially. This would speed up the process of generating a permutation distribution. 

Still, permutation tests are useful for smaller sample sizes. While so much of today's data comes in large data sets, there are settings in which we need to draw inferences from small sample sizes, such as medical studies with a complex or costly procedure or for a rare disease where it is difficult to recruit a large pool of subjects. Another example is in comparing the performance of large machine learning or AI models where training such models may take days or longer, so training these models many times to produce a large data set upon which to draw inferences is impractical.

# Notes

Code for this project can be found at www.github.com/mvered/permutation_tests.