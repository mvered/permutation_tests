% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Permutation Test Based U-Statistics: Simulations of Type I and Type II Error Rates},
  pdfauthor={Michelle Vered, Johns Hopkins University},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Permutation Test Based U-Statistics: Simulations of Type I and
Type II Error Rates}
\author{Michelle Vered, Johns Hopkins University}
\date{January 2026}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

This project explores the use of permutation tests in the context of
two-sample and independence testing. As described in the core paper
which is the basis for this project (Kim, Balakrishnan, and Wasserman
(2022)), permutation tests can provide finite sample guarantees on the
Type I error rate and can be minimax optimal in a variety of
circumstances. In particular, we focus on the use of permutation test
based \(U\)-statistics as applied to a two-sample comparison of medians
as well as in independence testing in high dimensional multinomial
problems.

We utilize a data set on housing prices as the basis for various
simulation studies to explore minimax optimality of permutation tests.
We divide this data set into two groups and use these groups to
represent the true population distributions of some random variable.

In one set of simulations, we sample from these two distinct
populations. We then compare the frequency of Type II error rates
(falsely failing to reject the null hypothesis) between a permutation
test and a non-permutation test based approach. In a second set of
simulations, we draw two samples both from the same underlying
population group. We then compare the prevalence of Type I errors
(falsely rejecting the null hypothesis) between permutation and
non-permutation test based approaches.

\section{Methods}\label{methods}

\subsection{Permutation Tests}\label{permutation-tests}

In hypothesis testing, we calculate a test statistic based on the sample
of data. Typically, we then assume that the under the null hypothesis,
that test statistic should follow some specific distribution, such as a
Student's \(t\)-distribution or a Chi-squared distribution. We choose a
critical value as some point from that distribution, such as the value
of the test statistic that corresponds to less than a 5\% probability of
observing a test statistic as extreme or more extreme than the critical
value if the null hypothesis were true. We make a decision about whether
to reject or fail to reject the null hypothesis by comparing our
observed test statistic to that critical value.

However, the standard repertoire of hypothesis tests do not work in all
situations. When dealing with small sample sizes, or when distributional
assumptions required for parametric tests are not met (such as
normality), one alternative we can turn to would be a permutation test.
Permutation tests are a flexible alternative that allow us to create our
own null distribution with which to compare our test statistic rather
than assuming a specific distribution.

The basic idea of permutation testing is to randomly re-shuffle the
labels of observations, then re-calculate the test statistic. This
process is repeated many times to create a distribution for the test
statistic under the null (Albert (2015)). If we are comparing two
samples, for example, if these two samples were drawn from the same
underlying population distribution (i.e.~the null was true), we would
expect that the labels (sample 1 or sample 2) would not matter, so we
want to calculate a permutation distribution for the test statistic
under that assumption. We then select a critical value for our decision
rule based on a quantile of the permutation distribution.

Formally, assume we have observations \(X=X_1,...,X_n\) and
\(Y=Y_1,...,Y_m\) and define a test statistic \(T(X,Y)\) which measures
the difference between the two samples. For each permutation, select
\(n\) observations from the pooled data \(X_1,...,X_n,Y_1,...,Y_m\) and
label these \[X^*_{(1)}=X_{(1)_1}^*,...,X_{(1)_n}^*\] and the remainder
of observations as \[Y^*_{(1)}=Y_{(1)_1}^*,...,Y_{(1)_n}^*\] then
calculate a permuted test statistic \(T^*_{(1)}(X^*,Y^*)\). Repeat this
process \(n\) times to get \[T^*_{(1)},...,T^*_{(n)}\] (or repeat many
times for a large sample where calculating every possible permutation of
the data is computationally unfeasible). Choose a critical value \(c^*\)
as a quantile from the distribution of \(T^*_{(1)},...,T^*_{(n)}\).

It is worth noting that bootstrapping is a related, but slightly
different, re-sampling procedure for creating a distribution of the test
statistic under the null. As compared with permutation testing, in a
bootstrap approach, we re-sample from the original data, with
replacement, to generate each new sample rather than permuting the
labels of the original data (Janssen and Pauls (2003)). However,
bootstrapping similarly creates a distribution for the test statistic
for which we can find a particular quantile to serve as the critical
value.

\subsection{Optimality}\label{optimality}

We explore several different approaches to hypothesis testing that allow
us to illustrate the performance of the permutation-based approach to
\(U\)-statistics. We focus on two applications in particular: testing
whether two numeric samples come from the same underlying population
with the same median (two-sample testing) and testing whether there is
an association between two categorical variables (independence testing).
In two sample testing, our null hypothesis is that the two samples come
from the same underlying distribution with the same median. In
independence testing, our null hypothesis is that there is no
association between the categorical variables.

To assess the performance of different approaches to hypothesis testing,
we are interested in measuring the Type I and Type II error rates
produced by our estimators. Type I error rates occur when the null
hypothesis is in fact true, but an estimator leads us to falsely reject
the null. Type II error rates occur when the null hypothesis is in fact
false, but an estimator fails to reject the null. The Type II error
rates may occur when the test is underpowered and fails to detect an
effect.

In practice, understanding the power of statistical test can be very
important, as statisticians are often called upon to help design studies
and determine optimal sample sizes for experiments or models to reach
sufficient statistical power. Overestimating the sample size needed can
lead to unneeded costs in the study implementation/data collection
phase, but underestimate the required sample size can result in wasted
effort running a study that produces only inconclusive results.

Previous work has explored the minimax optimality of permutation tests
in large samples (Robinson (1952)) and for simple test statistics such
as the \(t\)-statistic and \(F\)-statistic (Janssen (2005)). A primary
contribution of the key paper for this project (Kim, Balakrishnan, and
Wasserman (2022)) is to provide a non-asymptotic framework to analyze
the minimax Type II error of the permutation test. Their framework is
particularly useful for small sample sizes. In one set of Monte Carlo
simulations, they focus on simulations with \(n_1=n_2=100\).

We can define a minimax optimal estimator as one the minimizes the
maximum risk. Formally, the maximum risk of an estimator
\(\widehat{\theta}\) can be expressed as
\[\bar{R}(\widehat{\theta})=\sup_{\theta} R(\theta, \widehat{\theta})\]
and a minimax optimal estimator will be one such that
\[\sup_{\theta}R(\theta,\widehat{\theta})=\inf_{\tilde{\theta}} \sup_\theta R(\theta, \tilde{\theta})\]
where the infimum is over all estimators \(\tilde{\theta}\) (Wasserman
(2010)).

In practice, a hypothesis testing approach would be minimax optimal in
terms of the Type II error rate when the decision rule it produces
minimizes the likelihood of failing to reject the null hypothesis when
the null is in fact false, as compared to all other decision rules. This
``worst case'' situation would be most likely to occur when there is a
very small difference between the expected distributions under the null
and alternate hypotheses.

As an added step in this project, we also run simulations to measure the
Type I error rate. Finite sample guarantees on the Type I error rate are
much more well-established, as long as the exchangeability assumption is
met. These results are covered in older elements of the literature on
permutation tests (Hoeffding (1952)).

In our simulations, we will use the King County, Washington housing
price data as if it were the underlying true population distribution, so
simulations will not necessarily be measuring the Type I and Type II
error rates under the worst case, closest possible degree of similarity
between the null and alternate hypotheses. We also won't be comparing
permutation test based approaches with every other possible hypothesis
test. But we will be able to measure the Type II minimax optimality of
permutation tests in the sense of looking across many different samples
from the population and looking at the frequency of Type II errors
across the ``unluckiest'' samples with the worst possible configuration
of observations selected to be in the sample.

\subsection{U-Statistics}\label{u-statistics}

Here we specifically focus on the use of permutation-based approaches
with a class of statistics known as \(U\)-statistics. We can define
\(U_n\) as a \(U\)-statistic with kernel \(h\) of degree \(m\) as
\[U_n = {n \choose m}^{-1} \sum_{(i_i,...,i_m)\in I_{n,m}} h(X_{i_1},...,X_{i_m})\]
where \(h\) is a symmetric function over \(m\) arguments and we are
averaging the outcomes \(h(X_{i_1},...,X_{i_m})\) across all possible
ordered tuples \(I_{n,m}=\{(i_1,...,i_m): 1\leq i_1 <...<i_m \leq n \}\)
(Chen (2005), Wasserman (2010)). Many estimators can be written to fit
the format of this class of statistics, such as the sample variance.

\subsection{Two Sample Testing}\label{two-sample-testing}

In two-sample testing, we consider two samples of numeric data. Under
the null hypothesis, we assume both samples come from the same
underlying population distribution.

There are many options for performing this type of test. \(t\)-tests are
a traditional, parametric approach but assume the underlying data is
normally distributed. The Mann-Whitney \(U\)-test, also called the
Wilcoxon rank-sum test, is a non-parametric alternative. The
Mann-Whitney \(U\)-test is a test of both distribution shape and
location (the central tendency/median). If we are able to assume that
one sample is a shifted version of the distribution of the other sample,
then the Mann-Whitney \(U\)-test is a test that directly allows us to
test for a difference in medians (Hart (2001)). The Mann-Whitney
\(U\)-statistic is calculated by first defining a rank function that
orders the data in both samples
\[r(x_i)=\text{rank of }x_i \text{ in }x_1,x_2,...,x_{n_X},y_1,y_2,...,y_{n_Y}\]
Then the test statistic is the smaller of
\[U_1=n_Xn_Y + \frac{n_X(n_X+1)}{2}-\sum_{i=1}^{n_X} r(x_i)\] and
\[U_2 = n_X n_Y + \frac{n_Y(n_Y+1)}{2}-\sum_{j=1}^{n_Y} r(y_j)\]

While the Mann-Whitney \(U\)-test is non-parametric in that it does not
assume a normal distribution or any other specific distribution for the
underlying data, in the standard test we do assume that the distribution
of the \(U\)-statistic under the null can be approximated by the normal
distribution. Critical values are then taken that correspond to
probabilities of the normal distribution. This approximation tends to
work with sufficiently large sample sizes.

For a permutation test based Mann-Whitney \(U\)-statistic, the
\(U\)-statistic itself would be calculated in exactly the same way, but
the reference distribution for finding the critical value would be
created through permutation.

\subsection{Independence Testing}\label{independence-testing}

In independence testing, we consider two categorical variables. Under
the null hypothesis, we assume there is no association between these
variables. Here we specifically look to a binary categorical variable
and a second categorical variable that has a large number of categories.

A standard approach to multinomial independence testing is to use the
Chi-squared test. However, this approach has a number of drawbacks. Kim,
Balakrishnan, and Wasserman (2022) highlight the particular utility of
permutation testing in high-dimensional multinomial settings, when the
the number of categories is large and may even be larger than the number
of observations in the sample. The drawbacks of traditional tests for
these high dimensional multinomial settings was investigated in
Balakrishnan and Wasserman (2018), who suggest permutation tests as a
potential solution to explore.

Berrett, Kontoyiannis, and Samworth (2020) separately also consider the
optimality of \(U\)-statistic based permutation tests for independence
testing and arrive at similar results for minimax optimality as Kim,
Balakrishnan, and Wasserman (2022) but by taking a different
mathematical approach using different assumptions on smoothness of
permutation distributions.

Kim, Balakrishnan, and Wasserman (2022) propose a new \(U\)-statistic
for independence testing in high dimensional multinomial settings. Let
\(p_y\) and \(p_z\) be multinomial distributions on a discrete domain
\(\mathbb{S}_d\) with kernel
\[h_{ts}(y_1,y_2;z_1,z_2)=g(y_1,y_2)+g(z_1,z_2)-g(y_1,z_2)-g(y_2,z_1)\]
defined by the following bivariate function
\[g_\text{Multi}(x,y)=\sum_{k=1}^d \mathbb{I}(x=k)\mathbb{I}(y=k)\] and
the corresponding \(U\)-statistic
\[U_{n_1,n_2}=\frac{1}{(n_1)_{(2)}(n_2)_{(2)}} \sum_{(i_1,i_2)\in i^{n_1}_2} \sum_{(j_1,j_2)\in i^{n_2}_2} h_{ts}(Y_{i_1},Y_{i_2};Z_{j_1},Z_{j_2})\]

A unique contribution of this project is implementing calculation of
this test statistic into C++/Rcpp.

\section{Data Exploration}\label{data-exploration}

\subsection{Overview}\label{overview}

We now turn to an exploration of the data set used for running the
simulations in this project.

The data set we have selected for this project provides information on
the sale price and other characteristics of houses which were sold in
King County, Washington (the Seattle area). The data set includes 21613
observations on homes sold between 2014 - 2015. Each observation in the
data set represents one sale of a property.

After importing, we applied some basic data cleaning by re-classifying
the data types for each of the columns when necessary and converted
prices to be in thousands of dollars. We also engineered two Boolean
variables to convert column that records the square footage of a
basement into a binary classification, and did the same for a column
indicating the year a house was renovated. Overall summary statistics
for this data set are presented below:

\begin{longtable}[]{@{}ll@{}}
\caption{Data summary}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
Name & data \\
Number of rows & 21613 \\
Number of columns & 23 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Column type frequency: & \\
character & 1 \\
Date & 1 \\
factor & 4 \\
logical & 3 \\
numeric & 14 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Group variables & None \\
\end{longtable}

\textbf{Variable type: character}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1944}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1389}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1944}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1528}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
min
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
max
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
empty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
whitespace
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
id & 0 & 1 & 7 & 10 & 0 & 21436 & 0 \\
\end{longtable}

\textbf{Variable type: Date}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1750}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1250}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1375}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
min
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
max
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
median
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
date & 0 & 1 & 2014-05-02 & 2015-05-27 & 2014-10-16 & 372 \\
\end{longtable}

\textbf{Variable type: factor}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1489}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1064}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1489}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0851}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0957}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.4149}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ordered
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
top\_counts
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
view & 0 & 1 & TRUE & 5 & 0: 19489, 2: 963, 3: 510, 1: 332 \\
condition & 0 & 1 & TRUE & 5 & 3: 14031, 4: 5679, 5: 1701, 2: 172 \\
grade & 0 & 1 & TRUE & 12 & 7: 8981, 8: 6068, 9: 2615, 6: 2038 \\
zipcode & 0 & 1 & FALSE & 70 & 981: 602, 980: 590, 981: 583, 980: 574 \\
\end{longtable}

\textbf{Variable type: logical}

\begin{longtable}[]{@{}lrrrl@{}}
\toprule\noalign{}
skim\_variable & n\_missing & complete\_rate & mean & count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
waterfront & 0 & 1 & 0.01 & FAL: 21450, TRU: 163 \\
has\_basement & 0 & 1 & 0.39 & FAL: 13126, TRU: 8487 \\
was\_renovated & 0 & 1 & 0.04 & FAL: 20699, TRU: 914 \\
\end{longtable}

\textbf{Variable type: numeric}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1728}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1235}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1728}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0741}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0988}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0617}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0617}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0617}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0741}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0988}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p25
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p50
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p75
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p100
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
price & 0 & 1 & 540 & 367.1 & 75 & 322 & 450 & 645 & 7700 \\
bedrooms & 0 & 1 & 3 & 0.9 & 0 & 3 & 3 & 4 & 33 \\
bathrooms & 0 & 1 & 2 & 0.8 & 0 & 2 & 2 & 2 & 8 \\
sqft\_living & 0 & 1 & 2080 & 918.4 & 290 & 1427 & 1910 & 2550 &
13540 \\
sqft\_lot & 0 & 1 & 15107 & 41420.5 & 520 & 5040 & 7618 & 10688 &
1651359 \\
floors & 0 & 1 & 1 & 0.5 & 1 & 1 & 2 & 2 & 4 \\
sqft\_above & 0 & 1 & 1788 & 828.1 & 290 & 1190 & 1560 & 2210 & 9410 \\
sqft\_basement & 0 & 1 & 292 & 442.6 & 0 & 0 & 0 & 560 & 4820 \\
yr\_built & 0 & 1 & 1971 & 29.4 & 1900 & 1951 & 1975 & 1997 & 2015 \\
yr\_renovated & 0 & 1 & 84 & 401.7 & 0 & 0 & 0 & 0 & 2015 \\
lat & 0 & 1 & 48 & 0.1 & 47 & 47 & 48 & 48 & 48 \\
long & 0 & 1 & -122 & 0.1 & -123 & -122 & -122 & -122 & -121 \\
sqft\_living15 & 0 & 1 & 1987 & 685.4 & 399 & 1490 & 1840 & 2360 &
6210 \\
sqft\_lot15 & 0 & 1 & 12768 & 27304.2 & 651 & 5100 & 7620 & 10083 &
871200 \\
\end{longtable}

These summary statistics show a relatively clean data set already, with
no missing values in any of the columns.

We will focus on only a subset of the columns available in this data
set. Of particular interest are the columns:

\begin{itemize}
\item
  \emph{has\_basement}: Boolean variable indicating if the house has a
  basement or not
\item
  \emph{price}: numeric variable indicating the sale price of the home,
  in thousands of dollars
\item
  \emph{zipcode}: categorical variable
\end{itemize}

Since this paper focuses on the application of permutation-test based
\(U\)-statistics in the context of two sample testing and independence
testing, we are interested in dividing the data set into two samples. We
can do that using the \emph{has\_basement} column. While
\emph{waterfront} is also a Boolean variable available in the data set,
the very small overall number of waterfront properties makes this column
less suitable for comparison of hypothesis testing approaches. We also
considered splitting the data based on the was \emph{was\_renovated}
column, but the \emph{has\_basement} column seemed to better fit the
assumption of a shifted distribution required for the Mann-Whitney
\(U\)-test, so we decided to focus there for the purpose of comparing
approaches to two-sample and independence testing.

Diving the data set based on homes with and without a basement, we have:

\begin{itemize}
\item
  \textbf{Population 1:} 8,487 properties with basements (39.3\% of the
  data set)
\item
  \textbf{Population 2:} 13,126 properties with basements (60.7\% of the
  data set)
\end{itemize}

\subsection{Price}\label{price}

We can compare quantiles for the distribution of prices between these
two types of properties.

\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
has\_basement & Price Q25 & Median Price & Price Q75 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
FALSE & 299 & 412 & 595 \\
TRUE & 375 & 515 & 712 \\
\end{longtable}

It seems clear from examining the quantiles of each population's
distribution that homes with and without basements represent two
distinct populations with different distributions of the price variable.

We can also visualize the whole distribution of prices for these two
groups of houses.

\includegraphics{report_files/figure-latex/histogram_basement_price-1.pdf}

There a few outliers - extremely expensive homes - which shift the
histograms and make it difficult to see prices clearly for houses under
\$2 million. It is also harder to directly compare the two distributions
since there are differing numbers of observations in the two groups.

To hone in on a comparison of price distributions, we can generate a new
overlapping density plot and exclude outliers (the largest 1\% of home
prices).

\includegraphics{report_files/figure-latex/density_plot_basement_price-1.pdf}

From this density plot, and the quantiles for each distribution provided
above, it appears as though the distribution of prices for homes with a
basement may be a shifted version of the distribution of prices for
homes without a basement. This is an ideal set-up for the use of a
Mann-Whitney \(U\)-test to compare medians, as discussed in more detail
in the methods section.

\subsection{Zip Codes}\label{zip-codes}

We can also compare the distribution of zip codes between the two
populations. Counts of homes per zip code can be thought of as draws
from a multinomial distribution.

To better understand how the distributions of zip codes might compare in
the two populations, homes with and without basements, we can examine a
bar chart. The bar chart below visualizes the percentage of homes in
each population that are located in a particular zip code.

\includegraphics{report_files/figure-latex/zip_by_basement-1.pdf}

It seems quite clear from this bar chart that the two populations follow
different underlying distributions. Since we are treating the data set
as our underlying populations, clear differences on the graph should be
enough to tell us about how zip code distributions compare between two
populations.

This data represents an ideal scenario to examine independence testing
for high dimensional multinomial distributions since there are 70
distinct zip codes included in the King County, Washington data set.

\section{Results}\label{results}

We now walk through the results of various simulation studies to assess
the performance of the permutation test approach.

\subsection{Two-Sample Testing}\label{two-sample-testing-1}

We first perform two-sample testing utilizing a Mann-Whitney
\(U\)-statistic to compare the median and shape of two distributions for
the price variable. 500 simulations and 500 permutations per simulation
were used to create the permutation distributions.

\subsubsection{Type II Error}\label{type-ii-error}

We start with data We'll start with a scenario where the null hypothesis
is actually false because we are drawing the two samples from separate
populations - homes with and without basements. For each simulation, we
record whether our test correctly rejects the null hypothesis or fails
to do so at a significance level of \(\alpha = 0.05\), checking both a
standard Mann-Whitney \(U\)-test and one utilizing a permutation
distribution.

`

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1512}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4070}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4419}}@{}}
\caption{Type II Error Rate of Two-Sample Testing Using a Mann-Whitney
U-Statistic}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Mann-Whitney U Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Permutation Mann-Whitney U Error Rate
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Mann-Whitney U Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Permutation Mann-Whitney U Error Rate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 100\% & 100\% \\
20 & 90\% & 100\% \\
30 & 80\% & 100\% \\
50 & 50\% & 100\% \\
100 & 10\% & 100\% \\
\end{longtable}

From these results, we see that the permutation based approach performs
similarly to the standard Mann-Whitney U test in terms of minimizing the
Type II error rate across all sample sizes.

\subsubsection{Type I Error}\label{type-i-error}

Now we run simulations where the null hypothesis should in fact be true.
We are drawing both samples from the same underlying population, where
houses do not have a basement.

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1512}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4070}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4419}}@{}}
\caption{Type I Error Rate of Two-Sample Testing Using a Mann-Whintey
U-Statistic}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Mann-Whitney U Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Permutation Mann-Whitney U Error Rate
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Mann-Whitney U Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Permutation Mann-Whitney U Error Rate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 0\% & 0\% \\
20 & 0\% & 0\% \\
30 & 0\% & 0\% \\
50 & 10\% & 0\% \\
100 & 40\% & 0\% \\
\end{longtable}

From these simulations, it appears the permutation test performs better
than the standard Mann-Whitney U test that uses a normal approximation
for the test statistic distribution terms of the Type I error rate
across all sample sizes.

The permutation based approach, at least for the relatively small sample
sizes considered, appears to match the performance of the standard
Mann-Whitney \(U\)-test as far as Type II error rates are concerned
while reducing Type I error rates. This highlights the value of the
permutation-based approach as a tool for two-sample hypothesis testing
in small sample sizes with non-normally distributed data.

\subsection{Independence Testing}\label{independence-testing-1}

Next we turn to independence testing. We are testing the null hypothesis
that there is no relationship between whether or not a house has a
basement and which zip code it is located in. This represents a
high-dimensional setting, given the high number of zip codes and
anticipated sample sizes we will study (\(n\leq 100\)).

The Chi-squared test can be used for this type of testing procedure.
However, p-values for the Chi-squared distribution assume certain
minimum expected counts per cell. In a larger Chi-squared table, the
test assumes that all expected counts must be at least 1 and no more
than 20\% of cells can have expected frequencies less than 5, or else
the p-value may not be valid. We anticipate this to be a major issue in
this circumstance and will likely lead to the presence of additional
Type I and Type II errors when the Chi-squared test is applied. We
perform the Chi-squared test anyway in order to highlight the utility of
the alternative permutation-based approach using a \(U\)-statistic
proposed by Kim, Balakrishnan, and Wasserman (2022).

For the sake of comparison, we also report out Type I and II error rates
for a Chi-squared test performed with simulated p-values (generated
through Monte Carlo simulation using a bootstrapping procedure).
Bootstrapping is similar to the permutation-based approach for
calculating p-values. However, the Chi-squared test with simulated
p-values still differs significantly from the Kim, Balakrishnan, and
Wasserman (2022). in that it is not a \(U\)-statistic and the test
statistic follows a different design/formula from the one specifically
proposed by Kim for high-dimensional multinomial testing. We include it
to provide another reference point for comparison of the new approach,
since p-values calculated through reference to the standard \(\chi^2\)
distribution are expected to be relatively inaccurate for this use case.

Due to computational limitations, the number of simulations, the number
of permutations, and the sample sizes tested are smaller than what we
ideally would like to test. 100 simulations were implemented with 100
permutations per simulation used to create the permutation. Some
parallelization has been implemented, but further simulations would
benefit from another look at parallelizing and optimizing the code as
well as cloud computing resources with a greater number of CPUs.

\subsubsection{Type II Errors}\label{type-ii-errors}

We now perform independence testing across a number of simulations,
comparing the Type II error rates generated through use of a \(\chi^2\)
test with standard p-values (calculated through reference to a
\(\chi^2\) distribution), a \(\chi^2\) test with simulated (Monte Carlo)
p-values, through the multinomial \(U\)-statistic permutation test
proposed Kim, Balakrishnan, and Wasserman (2022).

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1130}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2870}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3217}}@{}}
\caption{Type II Error Rate of Multinomial Independence
Testing}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bootstrap Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multinomial U Permutation Error Rate
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bootstrap Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multinomial U Permutation Error Rate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 100\% & 100\% & 100\% \\
20 & 100\% & 100\% & 100\% \\
30 & 100\% & 100\% & 100\% \\
50 & 90\% & 100\% & 100\% \\
100 & 80\% & 100\% & 100\% \\
\end{longtable}

The error rate for all methods is high, which is predictable considering
this is a high-dimensional multinomial setting and the sample sizes are
quite small. No matter the method, it is difficult to correctly detect
from just 10 or 20 observations that distribution of zip codes is
related to having a basement, since there are 70 distinct zip codes
included in the King County, Washington data set. We would expect the
prevalence of Type II errors to decline across all methods as the sample
size increases.

However, from these limited simulations, it appears the permutation test
performs better than the Chi-squared test (both versions) in terms of
the Type II error rate.

\subsubsection{Type I Errors}\label{type-i-errors}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1130}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2870}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3217}}@{}}
\caption{Type I Error Rate of Multinomial Independence
Testing}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bootstrap Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multinomial U Permutation Error Rate
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Observations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bootstrap Chi-Squared Error Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multinomial U Permutation Error Rate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 0\% & 0\% & 0\% \\
20 & 0\% & 0\% & 0\% \\
30 & 0\% & 0\% & 0\% \\
50 & 20\% & 0\% & 0\% \\
100 & 10\% & 0\% & 0\% \\
\end{longtable}

From these simulations, it appears the permutation test performs
similarly (or perhaps slightly worse) than the Chi-squared tests. More
precision in these estimates might be possible with a larger number of
simulations.

\section{Conclusions}\label{conclusions}

Our simulations show that the permutation-test based approach is a
useful tool when it comes to hypothesis testing for small sample sizes.
The theoretical guarantees on error rates given by Kim et al.~are
evident in the results of the simulations.

We first looked at two-sample testing, comparing the medians of two
numeric samples of data. At least for the small sample sizes considered,
the Type II error rate for p-values obtained through permutation versus
through approximation to the normal distribution are roughly similar.
However, the permutation approach appeared to reduce Type I error rates.
This highlights the value of the permutation-based approach as a tool
for two-sample hypothesis testing in small sample sizes with
non-normally distributed data.

We then looked at independence testing for two categorical variables. We
try to infer a general trend from the few sample sizes examined within
the scope of this paper and identify the multinomial \(U\)-statistic as
a potentially viable option for independence testing with categorical
variables.

However, we do note that permutation testing has some drawbacks, namely
the computational complexity. Permutation testing is not a very
practical approach for larger data sets due to the time required to run.
Even here with very small sample sizes, the simulations took a long time
to run, particularly when calculating the multinomial \(U\)-statistic
proposed by Kim et al.~This \(U\)-statistic was very slow to compute, a
permutation distribution of this statistic even more so. Future
implementations of that statistic might wish to consider optimizations
to code to better take advantage of parallelization and also to use
cloud computing tools with a larger number of CPUs, since permuted test
statistics can be calculated simultaneously rather than sequentially.
This would speed up the process of generating a permutation
distribution.

Still, permutation tests are useful for smaller sample sizes. While so
much of today's data comes in large data sets, there are settings in
which we need to draw inferences from small sample sizes, such as
medical studies with a complex or costly procedure or for a rare disease
where it is difficult to recruit a large pool of subjects. Another
example is in comparing the performance of large machine learning or AI
models where training such models may take days or longer, so training
these models many times to produce a large data set upon which to draw
inferences is impractical.

\section{Notes}\label{notes}

Code for this project can be found at
www.github.com/mvered/permutation\_tests.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-albert_melisande_tests_2015}
Albert, Melisande. 2015. {``Tests of Independence by Bootstrap and
Permutation : An Asymptotic and Non-Asymptotic Study. Application to
Neurosciences.''} PhD thesis, Universit√© Nice Sophia Antipolis.
\url{https://theses.hal.science/tel-01274647/file/2015NICE4079.pdf}.

\bibitem[\citeproctext]{ref-balakrishnan_hypothesis_2018}
Balakrishnan, Sivaraman, and Larry Wasserman. 2018. {``Hypothesis
Testing for High-Dimensional Multinomials: A Selective Review.''}
\emph{The Annals of Applied Statistics} 12 (2): 727--49.
\url{https://doi.org/10.1214/18-AOAS1155SF}.

\bibitem[\citeproctext]{ref-berrett_optimal_2020}
Berrett, Thomas B., Ioannis Kontoyiannis, and Richard J. Samworth. 2020.
{``Optimal Rates for Independence Testing via \$u\$-Statistic
Permutation Tests.''} {arXiv}.
\url{https://doi.org/10.48550/arXiv.2001.05513}.

\bibitem[\citeproctext]{ref-chen_u-statistics_2005}
Chen, Xiaohui. 2005. {``U-Statistics.''} \emph{Encyclopedia of
Biostatistics}.
\url{https://publish.illinois.edu/xiaohuichen/files/2018/11/stat05986_Chen_v3_nohighlight.pdf}.

\bibitem[\citeproctext]{ref-hart_mann_whitney_2001}
Hart, A. 2001. {``Mann-{Whitney} Test Is Not Just a Test of Medians:
Differences in Spread Can Be Important.''} \emph{BMJ (Clinical Research
Ed.)} 323 (7309): 391--93.
\url{https://doi.org/10.1136/bmj.323.7309.391}.

\bibitem[\citeproctext]{ref-hoeffding_large-sample_1952}
Hoeffding, Wassily. 1952. {``The Large-Sample Power of Tests Based on
Permutations of Observations.''} \emph{The Annals of Mathematical
Statistics} 23 (2): 169--92.
\url{https://doi.org/10.1214/aoms/1177729436}.

\bibitem[\citeproctext]{ref-janssen_resampling_2005}
Janssen, Arnold. 2005. {``Resampling Student's t-Type Statistics.''}
\emph{Annals of the Institute of Statistical Mathematics} 57 (February):
507--29. \url{https://doi.org/10.1007/BF02509237}.

\bibitem[\citeproctext]{ref-janssen_how_2003}
Janssen, Arnold, and Thorsten Pauls. 2003. {``How Do Bootstrap and
Permutation Tests Work?''} \emph{The Annals of Statistics} 31 (3):
768--806. \url{https://doi.org/10.1214/aos/1056562462}.

\bibitem[\citeproctext]{ref-kim_minimax_2022}
Kim, Ilmun, Sivaraman Balakrishnan, and Larry Wasserman. 2022.
{``Minimax Optimality of Permutation Tests.''} \emph{The Annals of
Statistics} 50 (1): 225--51. \url{https://doi.org/10.1214/21-AOS2103}.

\bibitem[\citeproctext]{ref-robinson_large-sample_1952}
Robinson, J. 1952. {``The Large-Sample Power of Permutation Tests for
Randomization Models.''} \emph{The Annals of Statistics} 1 (2): 291--96.
\url{https://www.jstor.org/stable/2958014}.

\bibitem[\citeproctext]{ref-wasserman_all_2010}
Wasserman, Larry. 2010. \emph{All of Statistics: A Concise Course in
Statistical Inference}. New York: Springer.

\end{CSLReferences}

\end{document}
